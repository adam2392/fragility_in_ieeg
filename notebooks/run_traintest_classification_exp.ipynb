{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Experiment - Retrospective Study with Neural Fragility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import os\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mne_bids.path import get_entities_from_fname\n",
    "from natsort import natsorted\n",
    "from rerf.rerfClassifier import rerfClassifier\n",
    "\n",
    "# comparative classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    balanced_accuracy_score,\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    brier_score_loss,\n",
    "    plot_precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.model_selection import GroupKFold, cross_validate\n",
    "from sklearn.utils import resample\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "import dabest\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "# functions related to the feature comparison experiment\n",
    "from analysis.publication.study import (\n",
    "    load_patient_dict,\n",
    "    determine_feature_importances,\n",
    "    extract_Xy_pairs,\n",
    "    format_supervised_dataset,\n",
    "    _sequential_aggregation,\n",
    "    tune_hyperparameters,\n",
    ")\n",
    "from analysis.publication.extract_datasets import load_ictal_frag_data\n",
    "from analysis.publication.utils import NumpyEncoder\n",
    "\n",
    "from sample_code.io import read_participants_tsv\n",
    "from sample_code.utils import _load_turbo, _plot_roc_curve\n",
    "\n",
    "%matplotlib inline\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_roc(fpr, tpr):\n",
    "    \"\"\"Compute average ROC statistics.\"\"\"\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 200)\n",
    "\n",
    "    n_splits = len(fpr)\n",
    "    print(f\"Computing average ROC over {n_splits} CV splits\")\n",
    "    for i in range(n_splits):\n",
    "        interp_tpr = np.interp(mean_fpr, fpr[i], tpr[i])\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(auc(mean_fpr, interp_tpr))\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    return mean_fpr, tprs, aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_patient_predictions(\n",
    "    ytrues, ypred_probs, subject_groups, pat_predictions=None, pat_true=None\n",
    "):\n",
    "    if pat_predictions is None or pat_true is None:\n",
    "        pat_predictions = collections.defaultdict(list)\n",
    "        pat_true = dict()\n",
    "\n",
    "    # loop through things\n",
    "    for ytrue, ypred_proba, subject in zip(ytrues, ypred_probs, subject_groups):\n",
    "        pat_predictions[subject].append(float(ypred_proba))\n",
    "\n",
    "        if subject not in pat_true:\n",
    "            pat_true[subject] = ytrue[0]\n",
    "        else:\n",
    "            if pat_true[subject] != ytrue[0]:\n",
    "                raise RuntimeError(\"wtf subject should all match...\")\n",
    "    return pat_predictions, pat_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# get line between optimum and clinical op point\n",
    "def create_line(x1, x2, y1, y2, n_points=200):\n",
    "    slope = (y2 - y1) / (x2 - x1)\n",
    "\n",
    "    xs = np.linspace(x1, x2, n_points)\n",
    "    ys = np.linspace(y1, y2, n_points)\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def find_intersect_idx(x1s, y1s, x2s, y2s):\n",
    "    \"\"\"Help find intersection points between two curves.\"\"\"\n",
    "    euc_dists = []\n",
    "    points = np.vstack((x2s, y2s)).T\n",
    "    for idx, (x1, y1) in enumerate(zip(x1s, y1s)):\n",
    "        point = np.array([x1, y1])[np.newaxis, :]\n",
    "        dists = cdist(points, point)\n",
    "        euc_dists.append(min(dists))\n",
    "    return np.argmin(euc_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed and randomness for downstream reproducibility\n",
    "seed = 12345\n",
    "random_state = 12345\n",
    "np.random.seed(seed)\n",
    "\n",
    "# proportion of subjects to use for training\n",
    "train_size = 0.6\n",
    "\n",
    "# classification model to use\n",
    "clf_type = \"mtmorf\"\n",
    "\n",
    "# BIDS related directories\n",
    "bids_root = Path(\"/Volumes/Seagate Portable Drive/data\")\n",
    "bids_root = Path(\"/Users/adam2392/Dropbox/epilepsy_bids/\")\n",
    "bids_root = Path(\"/home/adam2392/hdd/Dropbox/epilepsy_bids/\")\n",
    "deriv_path = bids_root / \"derivatives\"\n",
    "source_path = bids_root / \"sourcedata\"\n",
    "\n",
    "# metadata table\n",
    "excel_fpath = source_path / \"organized_clinical_datasheet_raw.xlsx\"\n",
    "\n",
    "# where to store the cross-validation indices to split patients on\n",
    "intermed_fpath = Path(deriv_path) / \"baselinesliced\"\n",
    "\n",
    "# where to save results\n",
    "study_path = Path(deriv_path) / \"study\"\n",
    "\n",
    "# feature names\n",
    "feature_names = [\n",
    "    \"fragility\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining evaluation criterion\n",
    "metric = \"roc_auc\"\n",
    "BOOTSTRAP = False\n",
    "\n",
    "# define hyperparameters\n",
    "windows = [\n",
    "    (-80, 25),\n",
    "]\n",
    "thresholds = [\n",
    "    0.5,\n",
    "    0.6,\n",
    "    0.7,\n",
    "]\n",
    "weighting_funcs = [None]\n",
    "\n",
    "max_depth = [None, 5, 10]\n",
    "max_features = [\"auto\", \"log2\"]\n",
    "IMAGE_HEIGHT = 20\n",
    "model_params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": max_depth[0],\n",
    "    \"\" \"max_features\": max_features[0],\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": random_state,\n",
    "}\n",
    "model_params.update(\n",
    "    {\n",
    "        #         \"projection_matrix\": \"S-RerF\",\n",
    "        \"projection_matrix\": \"MT-MORF\",\n",
    "        \"image_height\": IMAGE_HEIGHT,\n",
    "        \"image_width\": np.abs(windows[0]).sum(),\n",
    "        \"patch_height_max\": 4,\n",
    "        \"patch_height_min\": 1,\n",
    "        \"patch_width_max\": 8,\n",
    "        \"patch_width_min\": 1,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs:  1\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14663678256107216531\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16907912039669404536\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15548494327529476631\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10759418688\n",
      "locality {\n",
      "  bus_id: 2\n",
      "  numa_node: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1571639952698376271\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:41:00.0, compute capability: 6.1\"\n",
      "]\n",
      "WARNING:tensorflow:From <ipython-input-25-de5f7f3f973f>:43: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\"GPUs: \", len(tf.config.experimental.list_physical_devices(\"GPU\")))\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "# CNN if tensorflow is installed\n",
    "# Build CNN model\n",
    "def _build_cnn():\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation=\"relu\",\n",
    "            input_shape=(IMAGE_HEIGHT, np.sum(windows), 1),\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dense(n_classes, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.AUC(multi_label=False)],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# print(device_lib.list_local_devices('GPU'))\n",
    "print(tf.test.is_gpu_available())\n",
    "# cnn = KerasClassifier(_build_cnn, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup for run\n",
    "names = {\n",
    "    #     \"Log. Reg\": \"blue\",\n",
    "    #     \"Lin. SVM\": \"firebrick\",\n",
    "    #     \"SVM\": \"purple\",\n",
    "    #         \"kNN\": \"black\",\n",
    "    #     \"RF\": \"#f86000\",\n",
    "    #     \"MLP\": \"green\",\n",
    "    #     \"xgb\": \"black\",\n",
    "    #     \"dummby\": \"gray\",\n",
    "    \"cnn\": \"red\",\n",
    "    #     \"srerf\": \"red\",\n",
    "    # \"mtmorf\": \"orange\"\n",
    "}\n",
    "\n",
    "ncores = -1\n",
    "num_runs = 1\n",
    "n_est = 500  # number of estimators\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(random_state=0, n_jobs=ncores, solver=\"liblinear\"),\n",
    "    #     LinearSVC(),\n",
    "    #     SVC(C=1.0, probability=True, kernel=\"rbf\", gamma=\"auto\", random_state=0),\n",
    "    #     RandomForestClassifier(n_estimators=n_est, max_features=\"auto\", n_jobs=ncores),\n",
    "    #     MLPClassifier(hidden_layer_sizes=(n_est,), random_state=0, max_iter=1000),\n",
    "    #     GradientBoostingClassifier(random_state=random_state),\n",
    "    #     DummyClassifier(strategy=\"most_frequent\", random_state=random_state)\n",
    "    #     rerfClassifier(**model_params),\n",
    "    # rerfClassifier(**model_params),\n",
    "    #     KerasClassifier(_build_cnn, verbose=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    feature_name,\n",
    "    deriv_path,\n",
    "    excel_fpath,\n",
    "    patient_aggregation_method=None,\n",
    "    intermed_fpath=None,\n",
    "    save_cv_indices: bool = False,\n",
    "):\n",
    "    print(f\"Loading data from {intermed_fpath}\")\n",
    "    # load unformatted datasets\n",
    "    # i.e. datasets without data-hyperparameters applied\n",
    "    if feature_name == \"fragility\":\n",
    "        if not intermed_fpath:\n",
    "            (\n",
    "                unformatted_X,\n",
    "                y,\n",
    "                subject_groups,\n",
    "                sozinds_list,\n",
    "                onsetwin_list,\n",
    "            ) = load_ictal_frag_data(deriv_path, excel_fpath=excel_fpath)\n",
    "        else:\n",
    "            (\n",
    "                unformatted_X,\n",
    "                y,\n",
    "                subject_groups,\n",
    "                sozinds_list,\n",
    "                onsetwin_list,\n",
    "            ) = load_ictal_frag_data(intermed_fpath, excel_fpath=excel_fpath)\n",
    "    else:\n",
    "        if not intermed_fpath:\n",
    "            feature_subject_dict = load_patient_dict(\n",
    "                deriv_path, feature_name, task=\"ictal\", subjects=subjects\n",
    "            )\n",
    "            # get the (X, y) tuple pairs\n",
    "            (\n",
    "                unformatted_X,\n",
    "                y,\n",
    "                sozinds_list,\n",
    "                onsetwin_list,\n",
    "                subject_groups,\n",
    "            ) = extract_Xy_pairs(\n",
    "                feature_subject_dict,\n",
    "                excel_fpath=excel_fpath,\n",
    "                patient_aggregation_method=patient_aggregation_method,\n",
    "                verbose=False,\n",
    "            )\n",
    "        else:\n",
    "            # get the (X, y) tuple pairs\n",
    "            feature_fpath = intermed_fpath / f\"{feature_name}_unformatted.npz\"\n",
    "\n",
    "            with np.load(feature_fpath, allow_pickle=True) as data_dict:\n",
    "                unformatted_X, y = data_dict[\"unformatted_X\"], data_dict[\"y\"]\n",
    "                sozinds_list, onsetwin_list, subject_groups = (\n",
    "                    data_dict[\"sozinds_list\"],\n",
    "                    data_dict[\"onsetwin_list\"],\n",
    "                    data_dict[\"subject_groups\"],\n",
    "                )\n",
    "    # get the dataset parameters loaded in\n",
    "    dataset_params = {\"sozinds_list\": sozinds_list, \"onsetwin_list\": onsetwin_list}\n",
    "\n",
    "    # format supervised learning datasets\n",
    "    # define preprocessing to convert labels/groups into numbers\n",
    "    enc = OrdinalEncoder()  # handle_unknown='ignore', sparse=False\n",
    "    #     subject_groups = enc.fit_transform(np.array(subjects)[:, np.newaxis])\n",
    "    y = enc.fit_transform(np.array(y)[:, np.newaxis])\n",
    "    subject_groups = np.array(subject_groups)\n",
    "\n",
    "    # create held-out test dataset\n",
    "    # create separate pool of subjects for testing dataset\n",
    "    # 1. Cross Validation Training / Testing Split\n",
    "    if save_cv_indices:\n",
    "        gss = GroupShuffleSplit(n_splits=10, train_size=0.5, random_state=random_state)\n",
    "        for jdx, (train_inds, test_inds) in enumerate(\n",
    "            gss.split(unformatted_X, y, subject_groups)\n",
    "        ):\n",
    "            # if jdx != 7:\n",
    "            #     continue\n",
    "            train_pats = np.unique(subject_groups[train_inds])\n",
    "            test_pats = np.unique(subject_groups[test_inds])\n",
    "            np.savez_compressed(\n",
    "                study_path / \"inds\" / f\"{feature_name}-srerf-{jdx}-inds.npz\",\n",
    "                train_inds=train_inds,\n",
    "                test_inds=test_inds,\n",
    "                train_pats=train_pats,\n",
    "                test_pats=test_pats,\n",
    "            )\n",
    "    return unformatted_X, y, subject_groups, dataset_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clf_validation(\n",
    "    clf_type,\n",
    "    clf_func,\n",
    "    unformatted_X,\n",
    "    y,\n",
    "    subject_groups,\n",
    "    dataset_params,\n",
    "    study_path,\n",
    "    windows,\n",
    "    thresholds,\n",
    "    weighting_funcs,\n",
    "):\n",
    "    #     if y.ndim != 1:\n",
    "    #         y = y.copy().squeeze()\n",
    "    #         y = y.copy[:, np.newaxis]\n",
    "\n",
    "    unformatted_X = unformatted_X.copy()\n",
    "    y = y.copy()\n",
    "    subject_groups = subject_groups.copy()\n",
    "\n",
    "    # run this without the above for a warm start\n",
    "    for jdx in range(0, 10):\n",
    "        cv_scores = collections.defaultdict(list)\n",
    "\n",
    "        with np.load(\n",
    "            # study_path / \"inds\" / 'clinical_complexity' / f\"{jdx}-inds.npz\",\n",
    "            study_path\n",
    "            / \"inds\"\n",
    "            / \"fixed_folds_subjects\"\n",
    "            / f\"fragility-srerf-{jdx}-inds.npz\",\n",
    "            allow_pickle=True,\n",
    "        ) as data_dict:\n",
    "            # train_inds, test_inds = data_dict[\"train_inds\"], data_dict[\"test_inds\"]\n",
    "            train_pats, test_pats = data_dict[\"train_pats\"], data_dict[\"test_pats\"]\n",
    "\n",
    "        # set train indices based on which subjects\n",
    "        train_inds = [\n",
    "            idx for idx, sub in enumerate(subject_groups) if sub in train_pats\n",
    "        ]\n",
    "        test_inds = [idx for idx, sub in enumerate(subject_groups) if sub in test_pats]\n",
    "\n",
    "        # note that training data (Xtrain, ytrain) will get split again\n",
    "        # testing dataset (held out until evaluation)\n",
    "        subjects_test = subject_groups[test_inds]\n",
    "        print(subjects_test)\n",
    "\n",
    "        if len(np.unique(y[test_inds])) == 1:\n",
    "            print(f\"Skipping group cv iteration {jdx} due to degenerate test set\")\n",
    "            continue\n",
    "\n",
    "        \"\"\"Run cross-validation.\"\"\"\n",
    "        window = windows[0]\n",
    "        threshold = thresholds[0]\n",
    "        weighting_func = weighting_funcs[0]\n",
    "        X_formatted, dropped_inds = format_supervised_dataset(\n",
    "            unformatted_X,\n",
    "            **dataset_params,\n",
    "            window=window,\n",
    "            threshold=threshold,\n",
    "            weighting_func=weighting_func,\n",
    "        )\n",
    "\n",
    "        # run cross-validation\n",
    "        # instantiate model\n",
    "        #         if clf_func == RandomForestClassifier:\n",
    "        #             # instantiate the classifier\n",
    "        #             clf = clf_func(**model_params)\n",
    "        #                 elif clf_func == rerfClassifier:\n",
    "        #                     model_params.update({\"image_width\": np.abs(window).sum()})\n",
    "        #                     clf = clf_func(**model_params)\n",
    "        #                 else:\n",
    "        clf = clf_func\n",
    "        print(\"Updated classifier: \", clf)\n",
    "\n",
    "        # perform CV using Sklearn\n",
    "        scoring_funcs = {\n",
    "            \"roc_auc\": roc_auc_score,\n",
    "            \"accuracy\": accuracy_score,\n",
    "            \"balanced_accuracy\": balanced_accuracy_score,\n",
    "            \"average_precision\": average_precision_score,\n",
    "        }\n",
    "\n",
    "        def dummy_cv(train, test):\n",
    "            yield train_inds, test_inds\n",
    "\n",
    "        n_samps = len(y)\n",
    "        if isinstance(clf, KerasClassifier):\n",
    "            print(X_formatted.shape)\n",
    "            X_formatted = X_formatted.reshape(n_samps, 20, np.sum(window), 1)\n",
    "            print(\"new shape: \", X_formatted.shape)\n",
    "\n",
    "        scores = cross_validate(\n",
    "            clf,\n",
    "            X_formatted,\n",
    "            y,\n",
    "            groups=subject_groups,\n",
    "            cv=dummy_cv(train_inds, test_inds),\n",
    "            scoring=list(scoring_funcs.keys()),\n",
    "            return_estimator=True,\n",
    "            return_train_score=True,\n",
    "        )\n",
    "\n",
    "        # get the best classifier based on pre-chosen metric\n",
    "        test_key = f\"test_{metric}\"\n",
    "        print(scores.keys())\n",
    "        print(scores)\n",
    "\n",
    "        # removing array like structure\n",
    "        scores = {key: val[0] for key, val in scores.items()}\n",
    "        estimator = scores.pop(\"estimator\")\n",
    "        print(\"Using estimator \", estimator)\n",
    "\n",
    "        # resample the held-out test data via bootstrap\n",
    "        test_sozinds_list = np.asarray(dataset_params[\"sozinds_list\"])[test_inds]\n",
    "        test_onsetwin_list = np.asarray(dataset_params[\"onsetwin_list\"])[test_inds]\n",
    "        # evaluate on the testing dataset\n",
    "        X_test, y_test = np.array(X_formatted)[test_inds, ...], np.array(y)[test_inds]\n",
    "        groups_test = np.array(subject_groups)[test_inds]\n",
    "\n",
    "        if BOOTSTRAP:\n",
    "            for i in range(500):\n",
    "                X_boot, y_boot, sozinds, onsetwins = resample(\n",
    "                    X_test,\n",
    "                    y_test,\n",
    "                    test_sozinds_list,\n",
    "                    test_onsetwin_list,\n",
    "                    n_samples=len(y_test),\n",
    "                )\n",
    "        else:\n",
    "            X_boot, y_boot = X_test.copy(), y_test.copy()\n",
    "\n",
    "        # evaluate on the test set\n",
    "        y_pred_prob = estimator.predict_proba(X_boot)[:, 1]\n",
    "        y_pred = estimator.predict(X_boot)\n",
    "\n",
    "        # store the actual outcomes and the predicted probabilities\n",
    "        cv_scores[\"validate_ytrue\"].append(list(y_test))\n",
    "        cv_scores[\"validate_ypred_prob\"].append(list(y_pred_prob))\n",
    "        cv_scores[\"validate_ypred\"].append(list(y_pred))\n",
    "        cv_scores[\"validate_subject_groups\"].append(list(groups_test))\n",
    "\n",
    "        # store ROC curve metrics on the held-out test set\n",
    "        fpr, tpr, thresholds = roc_curve(y_boot, y_pred_prob, pos_label=1)\n",
    "        fnr, tnr, neg_thresholds = roc_curve(y_boot, y_pred_prob, pos_label=0)\n",
    "        cv_scores[\"validate_fpr\"].append(list(fpr))\n",
    "        cv_scores[\"validate_tpr\"].append(list(tpr))\n",
    "        cv_scores[\"validate_fnr\"].append(list(fnr))\n",
    "        cv_scores[\"validate_tnr\"].append(list(tnr))\n",
    "        cv_scores[\"validate_thresholds\"].append(list(thresholds))\n",
    "\n",
    "        print(\"Done analyzing ROC stats...\")\n",
    "\n",
    "        # run the feature importances\n",
    "        # compute calibration curve\n",
    "        try:\n",
    "            fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "                y_boot, y_pred_prob, n_bins=10, strategy=\"quantile\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                print(e)\n",
    "                fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "                    y_boot, y_pred_prob, n_bins=5, strategy=\"uniform\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                #         finally:\n",
    "                #             print(e)\n",
    "                fraction_of_positives = [None]\n",
    "                mean_predicted_value = [None]\n",
    "        clf_brier_score = np.round(\n",
    "            brier_score_loss(y_boot, y_pred_prob, pos_label=np.array(y_boot).max()), 2\n",
    "        )\n",
    "\n",
    "        print(\"Done analyzing calibration stats...\")\n",
    "\n",
    "        # store ingredients for a calibration curve\n",
    "        cv_scores[\"validate_brier_score\"].append(float(clf_brier_score))\n",
    "        cv_scores[\"validate_fraction_pos\"].append(list(fraction_of_positives))\n",
    "        cv_scores[\"validate_mean_pred_value\"].append(list(mean_predicted_value))\n",
    "\n",
    "        # store outputs to run McNemars test and Cochrans Q test\n",
    "        # get the shape of a single feature \"vector\" / structure array\n",
    "        pat_predictions, pat_true = combine_patient_predictions(\n",
    "            y_boot, y_pred_prob, subjects_test\n",
    "        )\n",
    "        cv_scores[\"validate_pat_predictions\"].append(pat_predictions)\n",
    "        cv_scores[\"validate_pat_true\"].append(pat_true)\n",
    "\n",
    "        # store output for feature importances\n",
    "        if clf_type == \"rf\":\n",
    "            n_jobs = -1\n",
    "        else:\n",
    "            n_jobs = 1\n",
    "        results = determine_feature_importances(\n",
    "            estimator, X_boot, y_boot, n_jobs=n_jobs\n",
    "        )\n",
    "        imp_std = results.importances_std\n",
    "        imp_vals = results.importances_mean\n",
    "        cv_scores[\"validate_imp_mean\"].append(list(imp_vals))\n",
    "        cv_scores[\"validate_imp_std\"].append(list(imp_std))\n",
    "\n",
    "        print(\"Done analyzing feature importances...\")\n",
    "\n",
    "        # save intermediate analyses\n",
    "        clf_func_path = (\n",
    "            study_path\n",
    "            / \"clf-train-vs-test\"\n",
    "            / \"classifiers\"\n",
    "            / f\"{clf_type}_classifiers_{feature_name}_{jdx}.npz\"\n",
    "        )\n",
    "        clf_func_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # nested CV scores\n",
    "        nested_scores_fpath = (\n",
    "            study_path\n",
    "            / \"clf-train-vs-test\"\n",
    "            / f\"study_cv_scores_{clf_type}_{feature_name}_{jdx}.json\"\n",
    "        )\n",
    "\n",
    "        # save the estimators\n",
    "        # if clf_type not in [\"srerf\", \"mtmorf\"]:\n",
    "        #    np.savez_compressed(clf_func_path, estimators=estimator)\n",
    "\n",
    "        # save all the master scores as a JSON file\n",
    "        with open(nested_scores_fpath, \"w\") as fin:\n",
    "            json.dump(cv_scores, fin, cls=NumpyEncoder)\n",
    "\n",
    "        del estimator\n",
    "        del scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Run Classification Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adam2392/hdd/Dropbox/epilepsy_bids/derivatives/study\n"
     ]
    }
   ],
   "source": [
    "print(study_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/adam2392/Dropbox/epilepsy_bids/derivatives/baselinesliced\n",
      "Got 94 subjects\n",
      "Got  431  datasets.\n",
      "Got  94  patients\n",
      "dict_keys(['jh101', 'jh103', 'jh105', 'jh107', 'jh108', 'la00', 'la01', 'la02', 'la03', 'la04', 'la05', 'la06', 'la07', 'la08', 'la09', 'la10', 'la11', 'la12', 'la13', 'la15', 'la16', 'la17', 'la20', 'la21', 'la22', 'la23', 'la24', 'la27', 'la28', 'la29', 'la31', 'nl01', 'nl03', 'nl04', 'nl05', 'nl07', 'nl08', 'nl09', 'nl10', 'nl13', 'nl14', 'nl15', 'nl16', 'nl17', 'nl18', 'nl19', 'nl20', 'nl21', 'nl22', 'nl23', 'nl24', 'pt1', 'pt2', 'pt3', 'pt6', 'pt7', 'pt8', 'pt10', 'pt11', 'pt12', 'pt13', 'pt14', 'pt15', 'pt16', 'pt17', 'tvb1', 'tvb2', 'tvb5', 'tvb7', 'tvb8', 'tvb11', 'tvb12', 'tvb14', 'tvb17', 'tvb18', 'tvb19', 'tvb23', 'tvb27', 'tvb28', 'tvb29', 'umf001', 'umf002', 'umf003', 'umf004', 'umf005', 'ummc001', 'ummc002', 'ummc003', 'ummc004', 'ummc005', 'ummc006', 'ummc007', 'ummc008', 'ummc009'])\n",
      "416 416 416 416 416\n"
     ]
    }
   ],
   "source": [
    "feature_name = \"fragility\"\n",
    "\n",
    "\n",
    "unformatted_X, y, subject_groups, dataset_params = load_data(\n",
    "    feature_name,\n",
    "    deriv_path,\n",
    "    excel_fpath,\n",
    "    patient_aggregation_method=None,\n",
    "    intermed_fpath=intermed_fpath,\n",
    "    save_cv_indices=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jh101' 'jh101' 'jh101' 'jh101' 'jh105' 'jh105' 'jh105' 'jh105' 'jh105'\n",
      " 'jh105' 'jh105' 'jh105' 'jh105' 'jh105' 'la00' 'la02' 'la05' 'la05'\n",
      " 'la05' 'la05' 'la05' 'la05' 'la05' 'la05' 'la05' 'la05' 'la05' 'la05'\n",
      " 'la05' 'la05' 'la05' 'la09' 'la09' 'la12' 'la12' 'la12' 'la12' 'la12'\n",
      " 'la12' 'la12' 'la12' 'la12' 'la15' 'la17' 'la20' 'la20' 'la20' 'la20'\n",
      " 'la20' 'la20' 'la20' 'la20' 'la21' 'la21' 'la21' 'la21' 'la21' 'la23'\n",
      " 'la23' 'la23' 'la24' 'la24' 'la24' 'la27' 'la27' 'la27' 'la27' 'la27'\n",
      " 'la27' 'la27' 'la27' 'la27' 'la27' 'la27' 'la27' 'la27' 'la27' 'la27'\n",
      " 'la27' 'la27' 'la27' 'la27' 'la27' 'la27' 'la27' 'la27' 'la27' 'la27'\n",
      " 'la28' 'la28' 'la28' 'la28' 'la28' 'la28' 'la28' 'la28' 'la28' 'la29'\n",
      " 'la29' 'la29' 'la29' 'la29' 'nl01' 'nl01' 'nl01' 'nl04' 'nl04' 'nl04'\n",
      " 'nl05' 'nl05' 'nl05' 'nl08' 'nl08' 'nl08' 'nl10' 'nl10' 'nl10' 'nl14'\n",
      " 'nl14' 'nl14' 'nl14' 'nl14' 'nl14' 'nl15' 'nl15' 'nl15' 'nl17' 'nl17'\n",
      " 'nl17' 'nl17' 'nl17' 'nl20' 'nl20' 'nl20' 'nl20' 'nl21' 'nl21' 'nl21'\n",
      " 'nl21' 'nl21' 'nl21' 'nl21' 'nl21' 'nl21' 'nl21' 'nl22' 'nl22' 'nl22'\n",
      " 'nl22' 'nl22' 'nl22' 'nl22' 'nl23' 'nl23' 'nl23' 'nl23' 'nl23' 'nl23'\n",
      " 'nl23' 'nl23' 'nl23' 'nl24' 'nl24' 'nl24' 'nl24' 'nl24' 'pt1' 'pt1' 'pt1'\n",
      " 'pt1' 'pt3' 'pt3' 'pt8' 'pt8' 'pt8' 'pt11' 'pt11' 'pt16' 'pt16' 'pt16'\n",
      " 'pt17' 'tvb5' 'tvb8' 'tvb8' 'tvb8' 'tvb8' 'tvb8' 'tvb8' 'tvb8' 'tvb8'\n",
      " 'tvb8' 'tvb8' 'tvb8' 'tvb8' 'tvb8' 'tvb11' 'tvb11' 'tvb11' 'tvb14'\n",
      " 'tvb14' 'tvb14' 'tvb14' 'tvb14' 'tvb14' 'tvb14' 'tvb14' 'tvb14' 'tvb14'\n",
      " 'tvb14' 'tvb14' 'tvb14' 'tvb14' 'tvb14' 'tvb17' 'tvb17' 'tvb17' 'tvb17'\n",
      " 'tvb18' 'tvb18' 'tvb18' 'tvb23' 'tvb23' 'tvb23' 'tvb23' 'tvb23' 'tvb23'\n",
      " 'tvb29' 'tvb29' 'tvb29' 'tvb29' 'tvb29' 'tvb29' 'tvb29' 'tvb29' 'tvb29'\n",
      " 'umf003' 'ummc002' 'ummc002' 'ummc002' 'ummc005' 'ummc005']\n",
      "Updated classifier:  LogisticRegression(n_jobs=-1, random_state=0, solver='liblinear')\n",
      "(416, 2100)\n",
      "new shape:  (416, 20, 105, 1)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "hi",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-cd5c4e73e99b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclf_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_func\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     run_clf_validation(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mclf_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mclf_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0munformatted_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-ab8375ec77a8>\u001b[0m in \u001b[0;36mrun_clf_validation\u001b[0;34m(clf_type, clf_func, unformatted_X, y, subject_groups, dataset_params, study_path, windows, thresholds, weighting_funcs)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mX_formatted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_formatted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_formatted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         scores = cross_validate(\n\u001b[1;32m     96\u001b[0m             \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: hi"
     ]
    }
   ],
   "source": [
    "for clf_name, clf_func in zip(names, classifiers):\n",
    "    run_clf_validation(\n",
    "        clf_name,\n",
    "        clf_func,\n",
    "        unformatted_X,\n",
    "        y,\n",
    "        subject_groups,\n",
    "        dataset_params,\n",
    "        study_path,\n",
    "        windows,\n",
    "        thresholds,\n",
    "        weighting_funcs,\n",
    "    )\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eztrack",
   "language": "python",
   "name": "eztrack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
